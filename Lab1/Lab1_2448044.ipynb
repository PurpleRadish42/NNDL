{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49204220",
   "metadata": {},
   "source": [
    "<p><center><h3> Lab-1: Implementation of Perceptron Network</h3></center>\n",
    "<center>R Abhijit Srivathsan - 2448044</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9a65",
   "metadata": {},
   "source": [
    "### **Program #1: Implementation of Simple neural network with activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448b60ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Products:\n",
      "x1*w1 = 0.0800\n",
      "x2*w2 = 0.1800\n",
      "x3*w3 = -0.0800\n",
      "bias*weight = 0.3500\n",
      "Net input = 0.5300\n",
      "\n",
      "Neuron Outputs:\n",
      "Binary Step Output = 1\n",
      "Bipolar Step Output = 1\n",
      "Binary Sigmoid Output = 0.6295\n",
      "Bipolar Sigmoid Output = 0.2590\n"
     ]
    }
   ],
   "source": [
    "# Inputs and weights\n",
    "x1 = 0.8\n",
    "x2 = 0.6\n",
    "x3 = 0.4\n",
    "w1 = 0.1\n",
    "w2 = 0.3\n",
    "w3 = -0.2\n",
    "bias_input = 1.0\n",
    "bias_weight = 0.35\n",
    "\n",
    "# Compute x*w products\n",
    "product1 = x1 * w1\n",
    "product2 = x2 * w2\n",
    "product3 = x3 * w3\n",
    "bias_product = bias_input * bias_weight\n",
    "\n",
    "# Net input\n",
    "net = product1 + product2 + product3 + bias_product\n",
    "\n",
    "# Activation functions\n",
    "def binary_step(net):\n",
    "    if net > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bipolar_step(net):\n",
    "    if net > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def binary_sigmoid(net):\n",
    "    import math\n",
    "    return 1 / (1 + math.exp(-net))\n",
    "\n",
    "def bipolar_sigmoid(net):\n",
    "    import math\n",
    "    return 2 / (1 + math.exp(-net)) - 1\n",
    "\n",
    "# Print intermediate results\n",
    "print('Intermediate Products:')\n",
    "print(f'x1*w1 = {product1:.4f}')\n",
    "print(f'x2*w2 = {product2:.4f}')\n",
    "print(f'x3*w3 = {product3:.4f}')\n",
    "print(f'bias*weight = {bias_product:.4f}')\n",
    "print(f'Net input = {net:.4f}\\n')\n",
    "\n",
    "# Print activation outputs\n",
    "print('Neuron Outputs:')\n",
    "print(f'Binary Step Output = {binary_step(net)}')\n",
    "print(f'Bipolar Step Output = {bipolar_step(net)}')\n",
    "print(f'Binary Sigmoid Output = {binary_sigmoid(net):.4f}')\n",
    "print(f'Bipolar Sigmoid Output = {bipolar_sigmoid(net):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401404d",
   "metadata": {},
   "source": [
    "### **Conclusion: Program #1 - Single Neuron with Multiple Activation Functions**\n",
    "\n",
    "Different activation functions yield different interpretations of a neuron's behavior.  \n",
    "- The **step functions** (binary and bipolar) provide discrete outputs suitable for classification tasks.  \n",
    "- The **sigmoid functions** (binary and bipolar) offer smooth, continuous outputs that are useful for gradient-based learning and probabilistic interpretation.  \n",
    "- In this experiment, all activations responded to the same net input (0.53) differently, showing how the choice of activation affects the output.  \n",
    "\n",
    "This helps in understanding how neurons behave in more complex networks and aids in selecting the right activation based on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a298c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21619843",
   "metadata": {},
   "source": [
    "### **Program #2: Implement perceptron network for AND function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ded8b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "x1=1, x2=1, target=1, output=-1, error=2, w1=2, w2=2, bias=2\n",
      "x1=1, x2=-1, target=-1, output=1, error=-2, w1=0, w2=4, bias=0\n",
      "x1=-1, x2=1, target=-1, output=1, error=-2, w1=2, w2=2, bias=-2\n",
      "x1=-1, x2=-1, target=-1, output=-1, error=0, w1=2, w2=2, bias=-2\n",
      "\n",
      "Epoch 2\n",
      "x1=1, x2=1, target=1, output=1, error=0, w1=2, w2=2, bias=-2\n",
      "x1=1, x2=-1, target=-1, output=-1, error=0, w1=2, w2=2, bias=-2\n",
      "x1=-1, x2=1, target=-1, output=-1, error=0, w1=2, w2=2, bias=-2\n",
      "x1=-1, x2=-1, target=-1, output=-1, error=0, w1=2, w2=2, bias=-2\n",
      "\n",
      "Training complete!\n",
      "Final weights: w1=2, w2=2, bias=-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input samples and targets\n",
    "inputs = [\n",
    "  [1,  1],\n",
    "  [1, -1],\n",
    "  [-1, 1],\n",
    "  [-1, -1]\n",
    "]\n",
    "targets = [1, -1, -1, -1]\n",
    "\n",
    "# Parameters\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "bias = 0\n",
    "theta = 0\n",
    "alpha = 1  # learning rate\n",
    "\n",
    "# Activation function (bipolar step)\n",
    "def activation(net):\n",
    "  if net > theta:\n",
    "      return 1\n",
    "  else:\n",
    "      return -1\n",
    "\n",
    "# Training loop\n",
    "epoch = 0\n",
    "converged = False\n",
    "while not converged:\n",
    "  epoch += 1\n",
    "  print(f\"\\nEpoch {epoch}\")\n",
    "  errors = 0\n",
    "\n",
    "  for i in range(len(inputs)):\n",
    "      x1, x2 = inputs[i]\n",
    "      t = targets[i]\n",
    "\n",
    "      # Calculate net input\n",
    "      net = x1 * w1 + x2 * w2 + bias\n",
    "\n",
    "      # Apply activation\n",
    "      y = activation(net)\n",
    "\n",
    "      # Calculate error\n",
    "      error = t - y\n",
    "\n",
    "      # Update weights if error\n",
    "      if error != 0:\n",
    "          w1 = w1 + alpha * error * x1\n",
    "          w2 = w2 + alpha * error * x2\n",
    "          bias = bias + alpha * error\n",
    "          errors += 1\n",
    "\n",
    "      print(f\"x1={x1}, x2={x2}, target={t}, output={y}, error={error}, w1={w1}, w2={w2}, bias={bias}\")\n",
    "\n",
    "  # Check for convergence\n",
    "  if errors == 0:\n",
    "      converged = True\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final weights: w1={w1}, w2={w2}, bias={bias}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e3e78",
   "metadata": {},
   "source": [
    "### **Conclusion: Program #2 - Perceptron Learning for Bipolar AND Function**\n",
    "\n",
    "The perceptron successfully learned the **bipolar AND function**, converging in just **2 epochs**.  \n",
    "- It correctly classified all input patterns by adjusting weights based on the error.  \n",
    "- The final values were:  \n",
    "  - `w1 = 2`  \n",
    "  - `w2 = 2`  \n",
    "  - `bias = -2`  \n",
    "\n",
    "This confirms the perceptron's ability to handle linearly separable problems like the AND function using a simple rule-based learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
