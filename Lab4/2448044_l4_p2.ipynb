{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f01974",
   "metadata": {},
   "source": [
    "<p><center><h3> Lab-4: Singleâ€‘Layer Neural Network Alarm System</h3></center>\n",
    "<center>R Abhijit Srivathsan - 2448044</center></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0079f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb32bc",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "This implementation demonstrates the fundamental concepts of neural network training:\n",
    "\n",
    "### Network Architecture\n",
    "- **Input Layer**: 2 neurons (xâ‚, xâ‚‚) representing sensor inputs\n",
    "- **Output Layer**: 1 neuron with sigmoid activation producing alarm signal\n",
    "- **Parameters**: 2 weights (wâ‚, wâ‚‚) and 1 bias (b)\n",
    "\n",
    "### Forward Propagation\n",
    "The network computes:\n",
    "1. **Linear combination**: z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + b\n",
    "2. **Activation**: Å· = Ïƒ(z) = 1/(1 + eâ»á¶»)\n",
    "\n",
    "### Loss Function\n",
    "**Mean Squared Error (MSE)**: E = (1/n)Î£(y - Å·)Â²\n",
    "\n",
    "### Learning Algorithm\n",
    "**Delta Rule** with batch gradient descent:\n",
    "- Weight update: w â† w - Î·(âˆ‚E/âˆ‚w)\n",
    "- Bias update: b â† b - Î·(âˆ‚E/âˆ‚b)\n",
    "- Learning rate: Î· = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abf93a",
   "metadata": {},
   "source": [
    "## 1. Training data\n",
    "\n",
    "For this scenario we assume that **any** sensor being active should raise the alarm (logical **OR**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d211000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: all 4 possible combinations\n",
    "data_x = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=float)\n",
    "\n",
    "# Expected outputs according to OR logic\n",
    "data_y = np.array([[0], [1], [1], [1]], dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be6a1a",
   "metadata": {},
   "source": [
    "### Truth Table for Alarm System\n",
    "\n",
    "The training data represents all possible combinations of sensor inputs with their expected alarm outputs:\n",
    "\n",
    "| xâ‚ (Motion) | xâ‚‚ (Door) | y (Alarm) | Logic |\n",
    "|-------------|-----------|-----------|-------|\n",
    "| 0           | 0         | 0         | No sensors active â†’ No alarm |\n",
    "| 0           | 1         | 1         | Door open â†’ Alarm |\n",
    "| 1           | 0         | 1         | Motion detected â†’ Alarm |\n",
    "| 1           | 1         | 1         | Both sensors â†’ Alarm |\n",
    "\n",
    "This implements **logical OR** behavior: the alarm should sound if **any** sensor detects activity. This is a linearly separable problem that a single-layer perceptron can solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7523114",
   "metadata": {},
   "source": [
    "## 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fa8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862ec30",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function\n",
    "\n",
    "The **sigmoid function** Ïƒ(z) = 1/(1 + eâ»á¶») has several important properties:\n",
    "\n",
    "- **Range**: (0, 1) - perfect for binary classification\n",
    "- **Differentiable**: Smooth gradient for backpropagation\n",
    "- **S-shaped curve**: Provides non-linear decision boundary\n",
    "- **Derivative**: Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z)) - computationally efficient\n",
    "\n",
    "The sigmoid derivative is used during backpropagation to compute gradients for weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283e318",
   "metadata": {},
   "source": [
    "## 3. Initialize weights and bias (small random values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0f6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:\n",
      " [[ 0.0497]\n",
      " [-0.0138]]\n",
      "Initial bias:\n",
      " [0.0648]\n"
     ]
    }
   ],
   "source": [
    "# two inputs -> one output\n",
    "w = np.random.randn(2, 1) * 0.1  # (2x1)\n",
    "b = np.random.randn(1) * 0.1      # scalar bias\n",
    "print(\"Initial weights:\\n\", w)\n",
    "print(\"Initial bias:\\n\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441097e",
   "metadata": {},
   "source": [
    "### ðŸ§¾ Initial Weights and Bias\n",
    "\n",
    "- **Initial Weights**:\n",
    "  \\[\n",
    "  W = $\\begin{bmatrix} 0.0497 \\\\ -0.0138 \\end{bmatrix}$\n",
    "  \\]\n",
    "\n",
    "- **Initial Bias**:\n",
    "  \\[\n",
    "  b = 0.0648\n",
    "  \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8ea22",
   "metadata": {},
   "source": [
    "### Weight Initialization Strategy\n",
    "\n",
    "**Small Random Initialization** is used here with the following rationale:\n",
    "\n",
    "- **Random values**: Break symmetry - if all weights start identical, they remain identical\n",
    "- **Small magnitude** (Ã—0.1): Prevents saturation of sigmoid function\n",
    "- **Normal distribution**: Provides balanced positive/negative initial values\n",
    "- **Reproducible**: `np.random.seed(42)` ensures consistent results\n",
    "\n",
    "The initial weights and bias will be small random values around zero, allowing the network to learn from gradient descent updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526b8a6",
   "metadata": {},
   "source": [
    "## 4. Forward pass â€“ predictions and intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd63f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (linear output):\n",
      " [[0.0648]\n",
      " [0.0509]\n",
      " [0.1144]\n",
      " [0.1006]]\n",
      "Å· (sigmoid output):\n",
      " [[0.5162]\n",
      " [0.5127]\n",
      " [0.5286]\n",
      " [0.5251]]\n",
      "\n",
      "Mean Squared Error before update = 0.23790376766405488\n"
     ]
    }
   ],
   "source": [
    "# Linear combination\n",
    "z = np.dot(data_x, w) + b  # (4x1)\n",
    "# Activation\n",
    "y_hat = sigmoid(z)\n",
    "print(\"z (linear output):\\n\", z)\n",
    "print(\"Å· (sigmoid output):\\n\", y_hat)\n",
    "\n",
    "# Error (Mean Squared Error)\n",
    "error = np.mean((data_y - y_hat) ** 2)\n",
    "print(\"\\nMean Squared Error before update =\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361dce6c",
   "metadata": {},
   "source": [
    "### âš™ï¸ Forward Pass Output & Error (Before Update)\n",
    "\n",
    "- **Linear Combination (z)**:\n",
    "  \\[\n",
    "  $z = X \\cdot W + b =\n",
    "  \\begin{bmatrix}\n",
    "  0.0648 \\\\\n",
    "  0.0509 \\\\\n",
    "  0.1144 \\\\\n",
    "  0.1006\n",
    "  \\end{bmatrix}$\n",
    "  \\]\n",
    "\n",
    "- **Sigmoid Activation (Å·)**:\n",
    "  \\[\n",
    "  $\\hat{y} = \\sigma(z) =\n",
    "  \\begin{bmatrix}\n",
    "  0.5162 \\\\\n",
    "  0.5127 \\\\\n",
    "  0.5286 \\\\\n",
    "  0.5251\n",
    "  \\end{bmatrix}$\n",
    "  \\]\n",
    "\n",
    "- **Mean Squared Error** (before update):  \n",
    "  \\[\n",
    "  $\\text{MSE} = 0.2379$\n",
    "  \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c5b01",
   "metadata": {},
   "source": [
    "### Forward Pass Analysis\n",
    "\n",
    "The forward pass computes predictions for all training samples simultaneously:\n",
    "\n",
    "1. **Linear Transformation**: z = Xw + b\n",
    "   - Matrix multiplication: (4Ã—2) Ã— (2Ã—1) + (1Ã—1) = (4Ã—1)\n",
    "   - Each row represents one training sample's linear output\n",
    "\n",
    "2. **Activation**: Å· = Ïƒ(z)\n",
    "   - Applies sigmoid element-wise to convert linear outputs to probabilities\n",
    "   - Values close to 0 indicate \"no alarm\", close to 1 indicate \"alarm\"\n",
    "\n",
    "3. **Error Calculation**: MSE = mean((y - Å·)Â²)\n",
    "   - Measures how far predictions are from true labels\n",
    "   - Higher MSE indicates poorer performance\n",
    "\n",
    "The initial predictions will likely be poor since weights are random, but this establishes our baseline before learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf3949",
   "metadata": {},
   "source": [
    "## 5. Backward pass â€“ update with delta rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c84a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t weights:\n",
      " [[-0.059]\n",
      " [-0.06 ]]\n",
      "Gradient w.r.t bias:\n",
      " -0.05717840777138376\n",
      "\n",
      "Updated weights:\n",
      " [[ 0.0556]\n",
      " [-0.0078]]\n",
      "Updated bias:\n",
      " [0.0705]\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of error w.r.t. y_hat\n",
    "error_grad = -(data_y - y_hat)        # d(MSE)/dÅ·  for each sample (4x1)\n",
    "# Gradient w.r.t. z\n",
    "z_grad = error_grad * sigmoid_derivative(y_hat)\n",
    "\n",
    "# Gradients for weights and bias (batch = mean of sample grads)\n",
    "w_grad = np.dot(data_x.T, z_grad) / len(data_x)\n",
    "b_grad = np.mean(z_grad)\n",
    "\n",
    "lr = 0.1\n",
    "w_new = w - lr * w_grad\n",
    "b_new = b - lr * b_grad\n",
    "\n",
    "print(\"Gradient w.r.t weights:\\n\", w_grad)\n",
    "print(\"Gradient w.r.t bias:\\n\", b_grad)\n",
    "print(\"\\nUpdated weights:\\n\", w_new)\n",
    "print(\"Updated bias:\\n\", b_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a612caa",
   "metadata": {},
   "source": [
    "### ðŸ” Backpropagation & Weight Update\n",
    "\n",
    "- **Gradient w.r.t. Weights**:\n",
    "  \\[\n",
    "  $\\frac{\\partial \\text{MSE}}{\\partial W} =\n",
    "  \\begin{bmatrix}\n",
    "  -0.059 \\\\\n",
    "  -0.060\n",
    "  \\end{bmatrix}$\n",
    "  \\]\n",
    "\n",
    "- **Gradient w.r.t. Bias**:\n",
    "  \\[\n",
    "  $\\frac{\\partial \\text{MSE}}{\\partial b} = -0.0572$\n",
    "  \\]\n",
    "\n",
    "- **Updated Weights** (after learning rate = 0.1):\n",
    "  \\[\n",
    "  $W_{\\text{new}} =\n",
    "  \\begin{bmatrix}\n",
    "  0.0556 \\\\\n",
    "  -0.0078\n",
    "  \\end{bmatrix}$\n",
    "  \\]\n",
    "\n",
    "- **Updated Bias**:\n",
    "  \\[\n",
    "  $b_{\\text{new}} = 0.0705$\n",
    "  \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60b3f7",
   "metadata": {},
   "source": [
    "### Backpropagation and Weight Update\n",
    "\n",
    "The **Delta Rule** implements gradient descent to minimize error:\n",
    "\n",
    "#### Step 1: Error Gradient\n",
    "- `error_grad = -(y - Å·)` - Derivative of MSE w.r.t. predictions\n",
    "- Negative sign because we want to minimize error\n",
    "\n",
    "#### Step 2: Local Gradient  \n",
    "- `z_grad = error_grad Ã— Ïƒ'(Å·)` - Chain rule application\n",
    "- Sigmoid derivative weights the error by activation sensitivity\n",
    "\n",
    "#### Step 3: Parameter Gradients\n",
    "- **Weight gradient**: `âˆ‚E/âˆ‚w = X^T Ã— z_grad / n` (averaged over batch)\n",
    "- **Bias gradient**: `âˆ‚E/âˆ‚b = mean(z_grad)` (averaged over batch)\n",
    "\n",
    "#### Step 4: Parameter Update\n",
    "- **New weights**: `w â† w - Î· Ã— âˆ‚E/âˆ‚w`\n",
    "- **New bias**: `b â† b - Î· Ã— âˆ‚E/âˆ‚b`\n",
    "- Learning rate Î· = 0.1 controls step size\n",
    "\n",
    "This single update should move the network toward better OR gate behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820cebfa",
   "metadata": {},
   "source": [
    "## 6. Verifying new error after one update (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb656d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Mean Squared Error = 0.23584379921143195\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with updated parameters\n",
    "z2 = np.dot(data_x, w_new) + b_new\n",
    "y_hat2 = sigmoid(z2)\n",
    "error2 = np.mean((data_y - y_hat2) ** 2)\n",
    "print(\"New Mean Squared Error =\", error2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcecd3",
   "metadata": {},
   "source": [
    "### âœ… Forward Pass After Update\n",
    "\n",
    "- **New Mean Squared Error**:\n",
    "  \\[\n",
    "  \\text{MSE}_{\\text{after update}} = 0.2358\n",
    "  \\]\n",
    "\n",
    "> ðŸ”» The error decreased from **0.2379** to **0.2358**, showing that the network has started learning and adjusting weights in the right direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399a47d",
   "metadata": {},
   "source": [
    "### Learning Progress Analysis\n",
    "\n",
    "After one epoch of training, we can observe:\n",
    "\n",
    "1. **Error Reduction**: The new MSE should be lower than the initial MSE, indicating learning\n",
    "2. **Weight Evolution**: Updated weights should better reflect the OR logic pattern\n",
    "3. **Prediction Improvement**: New predictions should be closer to desired outputs\n",
    "\n",
    "#### Expected Behavior:\n",
    "- For input [0,0]: prediction should move toward 0 (no alarm)\n",
    "- For inputs [0,1], [1,0], [1,1]: predictions should move toward 1 (alarm)\n",
    "\n",
    "#### Convergence Notes:\n",
    "- Single-layer networks can perfectly learn linearly separable patterns like OR\n",
    "- Multiple epochs would be needed for complete convergence\n",
    "- The delta rule guarantees convergence for linearly separable problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6521f5",
   "metadata": {},
   "source": [
    "### ðŸ§  Final Inference & Conclusion\n",
    "\n",
    "After performing one forward and backward pass on a single-layer neural network with a sigmoid activation:\n",
    "\n",
    "- The model learned to reduce its prediction error by adjusting weights and bias.\n",
    "- A clear reduction in Mean Squared Error (MSE) confirms that backpropagation worked as expected.\n",
    "- This demonstrates the basic principle of learning in neural networks using gradient descent and the delta rule.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ Error Comparison Table\n",
    "\n",
    "| Step                         | MSE           | Weights                             | Bias     |\n",
    "|-----------------------------|---------------|--------------------------------------|----------|\n",
    "| **Before Update**           | **0.2379**    | \\[\\[0.0497\\], \\[-0.0138\\]\\]         | 0.0648   |\n",
    "| **After One Update Step**   | **0.2358**    | \\[\\[0.0556\\], \\[-0.0078\\]\\]         | 0.0705   |\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Conclusion**:  \n",
    "The decrease in MSE indicates successful learning. This basic experiment confirms that a single-layer neural network can learn to map inputs to outputs by minimizing error using gradient descent and backpropagation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
